{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def estimate_advantages(rewards, values, gamma, tau):\n",
    "    tensor_type = type(rewards)\n",
    "    returns = tensor_type(rewards.size(0), 1)\n",
    "    deltas = tensor_type(rewards.size(0), 1)\n",
    "    advantages = tensor_type(rewards.size(0), 1)\n",
    "\n",
    "    prev_return = 0\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        returns[i] = rewards[i] + gamma * prev_return\n",
    "        deltas[i] = rewards[i] + gamma * prev_value - values[i]\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage\n",
    "\n",
    "        prev_return = returns[i, 0]\n",
    "        prev_value = values[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "def compute_discount_returns(rewards, discount):\n",
    "    \"\"\"given a list of returns and discount factor(gamma), compute the discount\n",
    "    returns.\n",
    "\n",
    "    Args:\n",
    "        returns(tensor of rewards):\n",
    "        discount(float):\n",
    "\n",
    "    Returns:\n",
    "        returns(list of floats)\n",
    "\n",
    "    \"\"\"\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    returns[-1] = rewards[-1]\n",
    "    for i in reversed(range(rewards.size(0)-1)):\n",
    "        returns[i] = returns[i+1]*discount + rewards[i]\n",
    "\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-23 00:57:58,364] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward_sum -1349.545995001862, reward_avg -1017.4772997500932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-554295e11f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# compute current logprob, this forward pass holds the parameters that we want to updata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;31m# compute action ratio, where ratio = new_log_prob / old_log_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_log_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-554295e11f53>\u001b[0m in \u001b[0;36mevaluate_action\u001b[0;34m(self, states, actions, volatile)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-554295e11f53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, volatile)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch0.3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \"\"\"\n\u001b[0;32m--> 807\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# policy, value func implementation\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, action_scale, hidden_act=F.tanh):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            input_dim:\n",
    "            hidden_size:\n",
    "            output_dim:\n",
    "            action_scale (float):\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mean_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.std_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.action_scale = action_scale\n",
    "        \n",
    "    def forward(self, x, volatile):\n",
    "        x = Variable(x, volatile=volatile)\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.hidden_act(self.w2(x))\n",
    "        mean = F.tanh(self.mean_head(x))*self.action_scale\n",
    "        std = F.softplus(self.std_head(x))\n",
    "        return mean, std\n",
    "    \n",
    "    def sample_action(self, states, volatile=True):\n",
    "        mean, std = self.forward(states, volatile)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return action.data\n",
    "    \n",
    "    def evaluate_action(self, states, actions, volatile=False):\n",
    "        mean, std = self.forward(states, volatile)\n",
    "        dist = Normal(mean, std)\n",
    "        logprob = dist.log_prob(Variable(actions))\n",
    "        return logprob\n",
    "    \n",
    "    \n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, hidden_act = F.relu):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.w3 = nn.Linear(hidden_size, output_dim)\n",
    "        self.hidden_act = hidden_act\n",
    "        \n",
    "    def forward(self, x, volatile):\n",
    "        x = Variable(x, volatile=volatile)\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.hidden_act(self.w2(x))\n",
    "        return self.w3(x)\n",
    "        \n",
    "    def predict(self, states, volatile=True):\n",
    "        return self.forward(states, volatile=volatile)      \n",
    "        \n",
    "        \n",
    "#--------------------------------------------------\n",
    "env = gym.make('Pendulum-v0')\n",
    "env._max_episode_steps = 1000\n",
    "env.seed(seed)\n",
    "# experimental stuff\n",
    "episodes = 10000\n",
    "show = 50\n",
    "gamma = 0.95\n",
    "tau = 0.95\n",
    "steps = 200\n",
    "render = False\n",
    "entropy_coefficient = 0.01\n",
    "\n",
    "R_list = []\n",
    "\n",
    "R_avg = -1000\n",
    "# logging\n",
    "state_log, action_log, reward_log, real_reward_log= [], [], [], []\n",
    "\n",
    "# ppo parameters\n",
    "ppo_epoch = 20\n",
    "ppo_epsilon = 0.2\n",
    "ppo_grad_norm = 0.5\n",
    "# baseline parameters\n",
    "baseline_epoch = 5\n",
    "baseline_grad_norm = 0.5\n",
    "\n",
    "# create policy and value function\n",
    "policy = GaussianPolicy(3, 128, 1, 2.0)\n",
    "p_optimizer = torch.optim.RMSprop(policy.parameters(), lr=1e-4)\n",
    "\n",
    "baseline = ValueFunction(3,128,1)\n",
    "b_optimizer = torch.optim.Adam(baseline.parameters(), lr=4e-4)\n",
    "\n",
    "for eps in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    del state_log[:]\n",
    "    del action_log[:]\n",
    "    del reward_log[:]\n",
    "    del real_reward_log[:]\n",
    "    \n",
    "    for step in range(steps):\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        action = policy.sample_action(torch.FloatTensor(state).view(1,-1))\n",
    "        next_state, reward, done, _ = env.step(np.asarray([action[0,0]]))\n",
    "        \n",
    "        state_log.append(torch.FloatTensor(state).view(1,-1))\n",
    "        action_log.append(action)\n",
    "        #reward_log.append(torch.FloatTensor([(reward+8)/8]))\n",
    "        reward_log.append(torch.FloatTensor([reward]))\n",
    "        real_reward_log.append(reward)\n",
    "                \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # update phase\n",
    "    States = torch.cat(state_log)\n",
    "    Actions = torch.cat(action_log)\n",
    "    Rewards = torch.cat(reward_log)\n",
    "    Rewards = (Rewards - Rewards.mean())/(Rewards.std()+1e-5)\n",
    "    Returns = compute_discount_returns(Rewards, gamma)\n",
    "    RealRewards = np.asarray(real_reward_log)\n",
    "    \n",
    "    # 1) PPO Update\n",
    "    # first step we update our policy via PPO\n",
    "    # before iterating through our update loop, let's compute the initial action logprob and advantage, note this\n",
    "    # old_log_prob is held constant thorugh our PPO update steps, so we treat it as a constant,\n",
    "    # i.e some_Variable_as_a_constant = Variable(some_Variable.data)\n",
    "    old_log_prob = Variable(policy.evaluate_action(States, Actions, volatile=True).data)\n",
    "    # compute advantage\n",
    "    Advantages = Variable(Returns).view(-1,1) - Variable(baseline.predict(States, volatile=True).data).view(-1,1)\n",
    "    # To do maybe normalize?\n",
    "    Advantages = (Advantages - Advantages.mean())/(Advantages.std()+1e-5)\n",
    "    for p in range(ppo_epoch):\n",
    "        # compute current logprob, this forward pass holds the parameters that we want to updata\n",
    "        logprob = policy.evaluate_action(States, Actions, volatile=False)\n",
    "        # compute action ratio, where ratio = new_log_prob / old_log_prob\n",
    "        ratio = torch.exp(logprob - old_log_prob)\n",
    "        # compute surrogate 1, which is surr = ratio * advantage\n",
    "        \n",
    "        surr1 = ratio*Advantages\n",
    "        # compute surrogate 2, which is the clipped value * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0-ppo_epsilon, 1.0+ppo_epsilon)*Advantages\n",
    "        # we want to maximize the clipped min between the two surrogates\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        # update\n",
    "        p_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.utils.clip_grad_norm(policy.parameters(), ppo_grad_norm)\n",
    "        p_optimizer.step()\n",
    "        \n",
    "    \n",
    "    # 2). Critic update\n",
    "    # for critic, we simply regress/fit our critic predictions to the actual return\n",
    "    for b in range(baseline_epoch):\n",
    "        # forwrad pass to get value prediction, volatile to false as we will be updating parameters in this forward\n",
    "        # pass \n",
    "        value_pred = baseline.predict(States, volatile=False)\n",
    "        # compute MSE between actual returns and value prediction\n",
    "        value_loss = (0.5*(Variable(Returns).view(-1,1) - value_pred.view(-1,1)).pow(2)).mean()\n",
    "        # update\n",
    "        b_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm(baseline.parameters(), baseline_grad_norm)\n",
    "        b_optimizer.step()\n",
    "        \n",
    "    # computing rewards\n",
    "    R_avg = 0.95*R_avg + 0.05*RealRewards.sum()\n",
    "    \n",
    "    R_list.append(R_avg)\n",
    "    \n",
    "    if R_avg >= -300.0:\n",
    "        print('reached good score after {} episodes, breaking!'.format(eps))\n",
    "        break\n",
    "        \n",
    "    \n",
    "    if eps % show == 0:\n",
    "        render = True\n",
    "        print('episode {}, reward_sum {}, reward_avg {}'.format(eps, RealRewards.sum(), R_avg))\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(R_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Below implementation uses Generalized Advantage estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy, value func implementation\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, action_scale, hidden_act=F.tanh):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            input_dim:\n",
    "            hidden_size:\n",
    "            output_dim:\n",
    "            action_scale (float):\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mean_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.std_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.action_scale = action_scale\n",
    "        \n",
    "    def forward(self, x, volatile):\n",
    "        x = Variable(x, volatile=volatile)\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.hidden_act(self.w2(x))\n",
    "        mean = F.tanh(self.mean_head(x))*self.action_scale\n",
    "        std = F.softplus(self.std_head(x))\n",
    "        return mean, std\n",
    "    \n",
    "    def sample_action(self, states, volatile=True):\n",
    "        mean, std = self.forward(states, volatile)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return action.data\n",
    "    \n",
    "    def evaluate_action(self, states, actions, volatile=False):\n",
    "        mean, std = self.forward(states, volatile)\n",
    "        dist = Normal(mean, std)\n",
    "        logprob = dist.log_prob(Variable(actions))\n",
    "        return logprob\n",
    "    \n",
    "    \n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, hidden_act = F.relu):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.w3 = nn.Linear(hidden_size, output_dim)\n",
    "        self.hidden_act = hidden_act\n",
    "        \n",
    "    def forward(self, x, volatile):\n",
    "        x = Variable(x, volatile=volatile)\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.hidden_act(self.w2(x))\n",
    "        return self.w3(x)\n",
    "        \n",
    "    def predict(self, states, volatile=True):\n",
    "        return self.forward(states, volatile=volatile)      \n",
    "        \n",
    "        \n",
    "#--------------------------------------------------\n",
    "env = gym.make('Pendulum-v0')\n",
    "env._max_episode_steps = 1000\n",
    "env.seed(seed)\n",
    "# experimental stuff\n",
    "episodes = 10000\n",
    "show = 50\n",
    "gamma = 0.95\n",
    "tau = 0.95\n",
    "steps = 200\n",
    "render = False\n",
    "entropy_coefficient = 0.01\n",
    "\n",
    "R_list = []\n",
    "\n",
    "R_avg = -1000\n",
    "# logging\n",
    "state_log, action_log, reward_log, real_reward_log= [], [], [], []\n",
    "\n",
    "# ppo parameters\n",
    "ppo_epoch = 20\n",
    "ppo_epsilon = 0.2\n",
    "ppo_grad_norm = 0.5\n",
    "# baseline parameters\n",
    "baseline_epoch = 5\n",
    "baseline_grad_norm = 0.5\n",
    "\n",
    "# create policy and value function\n",
    "policy = GaussianPolicy(3, 128, 1, 2.0)\n",
    "p_optimizer = torch.optim.RMSprop(policy.parameters(), lr=1e-4)\n",
    "\n",
    "baseline = ValueFunction(3,128,1)\n",
    "b_optimizer = torch.optim.Adam(baseline.parameters(), lr=4e-4)\n",
    "\n",
    "for eps in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    del state_log[:]\n",
    "    del action_log[:]\n",
    "    del reward_log[:]\n",
    "    del real_reward_log[:]\n",
    "    \n",
    "    for step in range(steps):\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        action = policy.sample_action(torch.FloatTensor(state).view(1,-1))\n",
    "        next_state, reward, done, _ = env.step(np.asarray([action[0,0]]))\n",
    "        \n",
    "        state_log.append(torch.FloatTensor(state).view(1,-1))\n",
    "        action_log.append(action)\n",
    "        #reward_log.append(torch.FloatTensor([(reward+8)/8]))\n",
    "        reward_log.append(torch.FloatTensor([reward]))\n",
    "        real_reward_log.append(reward)\n",
    "                \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # update phase\n",
    "    States = torch.cat(state_log)\n",
    "    Actions = torch.cat(action_log)\n",
    "    Rewards = torch.cat(reward_log)\n",
    "    Rewards = (Rewards - Rewards.mean())/(Rewards.std()+1e-5)\n",
    "    Returns = compute_discount_returns(Rewards, gamma)\n",
    "    RealRewards = np.asarray(real_reward_log)\n",
    "    \n",
    "    # 1) PPO Update\n",
    "    # first step we update our policy via PPO\n",
    "    # before iterating through our update loop, let's compute the initial action logprob and advantage, note this\n",
    "    # old_log_prob is held constant thorugh our PPO update steps, so we treat it as a constant,\n",
    "    # i.e some_Variable_as_a_constant = Variable(some_Variable.data)\n",
    "    old_log_prob = Variable(policy.evaluate_action(States, Actions, volatile=True).data)\n",
    "    # compute advantage\n",
    "    # first we need value prediction\n",
    "    value_pred_old = baseline.predict(States, volatile=True).data\n",
    "    Advantages, _= estimate_advantages(Rewards.view(-1,1),value_pred_old.view(-1,1),gamma, tau)\n",
    "    Advantages = Variable(Advantages)\n",
    "    for p in range(ppo_epoch):\n",
    "        # compute current logprob, this forward pass holds the parameters that we want to updata\n",
    "        logprob = policy.evaluate_action(States, Actions, volatile=False)\n",
    "        # compute action ratio, where ratio = new_log_prob / old_log_prob\n",
    "        ratio = torch.exp(logprob - old_log_prob)\n",
    "        # compute surrogate 1, which is surr = ratio * advantage\n",
    "        surr1 = ratio*Advantages\n",
    "        # compute surrogate 2, which is the clipped value * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0-ppo_epsilon, 1.0+ppo_epsilon)*Advantages\n",
    "        # we want to maximize the clipped min between the two surrogates\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        # update\n",
    "        p_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.utils.clip_grad_norm(policy.parameters(), ppo_grad_norm)\n",
    "        p_optimizer.step()\n",
    "        \n",
    "    \n",
    "    # 2). Critic update\n",
    "    # for critic, we simply regress/fit our critic predictions to the actual return\n",
    "    for b in range(baseline_epoch):\n",
    "        # forwrad pass to get value prediction, volatile to false as we will be updating parameters in this forward\n",
    "        # pass \n",
    "        value_pred = baseline.predict(States, volatile=False)\n",
    "        # compute MSE between actual returns and value prediction\n",
    "        value_loss = (0.5*(Variable(Returns).view(-1,1) - value_pred.view(-1,1)).pow(2)).mean()\n",
    "        # update\n",
    "        b_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm(baseline.parameters(), baseline_grad_norm)\n",
    "        b_optimizer.step()\n",
    "        \n",
    "    # computing rewards\n",
    "    R_avg = 0.95*R_avg + 0.05*RealRewards.sum()\n",
    "    \n",
    "    R_list.append(R_avg)\n",
    "    \n",
    "    if R_avg >= -300.0:\n",
    "        print('reached good score after {} episodes, breaking!'.format(eps))\n",
    "        break\n",
    "        \n",
    "    \n",
    "    if eps % show == 0:\n",
    "        render = True\n",
    "        print('episode {}, reward_sum {}, reward_avg {}'.format(eps, RealRewards.sum(), R_avg))\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rewards = torch.ones(10,1)\n",
    "value_pred = torch.rand(10,1)\n",
    "estimate_advantages(Rewards,value_pred, 0.99,0.95 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch0.3",
   "language": "python",
   "name": "pytorch0.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
