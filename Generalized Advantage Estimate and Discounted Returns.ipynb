{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Advantage Estimate and Discounted Returns\n",
    "\n",
    "The function call for Generalized Advantage Estimate (GAE)/discounted returns is usually in the form of :\n",
    "\n",
    "```python\n",
    "def compute_discount(rewards, gamma, lamda, masks, next_value):\n",
    "    ...\n",
    "```\n",
    "The function inputs are:\n",
    "* **rewards**: a list containing collected rewards.\n",
    "* **gamma**: discount factor $\\gamma \\in [0,1.0]$. Usually set at 0.9, 0.95, 0.99.\n",
    "* **lamda**: weighting factor $\\lambda \\in [0, 1.0]$ for GAE. Usually set at 0.95\n",
    "* **masks**: a list containing masking for termination of episode. We set masks to be 0 if done, else set to 1.0.\n",
    "* **next_value**: next value prediction provided so we can use it to bootstrap our computation of discount returns. See below for more details.\n",
    "\n",
    "### Example: CartPole\n",
    "Let's see how this works, Let's take an arbitrary example using OpenAI Gym's CartPole. Say we ran our agent in the CartPole environment for 10 steps, resulting in the following collected data:\n",
    "```python\n",
    "num_steps = 10\n",
    "gamma = 1.0\n",
    "lam = 1.0\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "masks = [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
    "```\n",
    "We can see from above that we collected data across two episodes (by looking at ```masks```). The first episode ran for 6 steps, terminating at ```masks[5]```. We then collected 4 more steps of data for the second episode (the environment resets internally when one episode terminates). **Note:** we don't know whether the last step collected above is the terminal step for the second episode or not. We will discuss this below.\n",
    "\n",
    "Let's first get a sense of what our returns should be. (Remember: **returns** is the **discounted sum of future rewards** from each time step). Given that we don't do any discounting (i.e ```gamma``` is set to 1.0), our returns for the first episode should be:\n",
    "```python\n",
    "return_for_episode_1 = [6.0, 5.0, 4.0, 3.0, 2.0, 1.0]\n",
    "```\n",
    "(Remember: **return** is the **discounted sum of future rewards** from each time step)\n",
    "\n",
    "What about for the next episode? Since we only collected 4 steps of the next episode, we can't see far enough to see whether the next episode only contains 4 steps or more. This is where **next_value** comes into play. **next_value** is used to bootstrap our return estimate.\n",
    "\n",
    "Imagine two scenarios, in the first scenario, the second episode terminates right after the 4 steps, so we get one more reward of [1.0] and the episode terminates. Then we know our return for second episode is:\n",
    "```python\n",
    "return_for_episode_1 = [6.0, 5.0, 4.0, 3.0, 2.0, 1.0]\n",
    "next_value = 1.0 # next step is terminal step, with last reward of 1.0\n",
    "return_for_episode_2 = [5.0, 4.0, 3.0, 2.0]\n",
    "returns_for_all_10_steps = [6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 5.0, 4.0, 3.0, 2.0]\n",
    "```\n",
    "In the second scenario, our 2nd episode goes for another 10 steps before termination.\n",
    "```python\n",
    "return_for_episode_1 = [6.0, 5.0, 4.0, 3.0, 2.0, 1.0]\n",
    "next_value = 10.0\n",
    "return_for_episode_2 = [14.0, 13.0, 12.0, 11.0]\n",
    "returns_for_all_10_steps = [6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 14.0, 13.0, 12.0, 11.0]\n",
    "```\n",
    "**Note**: In both scenarios above, we assumed to know exactly how many further steps and rewards recieved before termination (by providing the exact **next_value**). In reality, we won't know during our data collection until we have completed the entire 2nd episode rollout. To be able to get an immediate estimate of the next_value, we often parameterize a value function ```V(s)``` that estimates the next_value, this will introduce some bias/error in our return estimate. But this is the price we pay for bootstrapping (i.e get an estimate of next_value without waiting to finish collecting data for the next episode).\n",
    "\n",
    "## Discounted Return Computation (First Without GAE)\n",
    "\n",
    "Below is code for computing discounted return from the above cartpole example. Note that we utilize our masks list to take into account episode termination. This will reset the return count (by setting return accumulated so far to 0, making the termination step's return effectly = reward for that step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_case_1 is [ 6.  5.  4.  3.  2.  1.  5.  4.  3.  2.] \n",
      "return_case_2 is [  6.   5.   4.   3.   2.   1.  14.  13.  12.  11.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_discounted_return(rewards, masks, gamma, next_value):\n",
    "    \"\"\"Compute discounted returns given a list of rewards, discount_factor, a list of termination masks, \n",
    "    and next value estiamte.\n",
    "    \n",
    "    \"\"\"\n",
    "    data_len = len(rewards) # get length of episode\n",
    "    returns = np.zeros(data_len+1) # create returns, with additional index for next_value\n",
    "    returns[-1] = next_value # set additional index as next_value\n",
    "    \n",
    "    for i in reversed(range(data_len)):\n",
    "        returns[i] = rewards[i] + gamma*returns[i+1]*masks[i]\n",
    "    # note that returns should exclude the additional index.\n",
    "    return returns[:-1]\n",
    "    \n",
    "    \n",
    "# CartPole Example\n",
    "num_steps = 10\n",
    "gamma = 1.0\n",
    "rewards = np.ones(num_steps) # reward of all 1.0s\n",
    "masks = np.ones(num_steps) # create our masks\n",
    "masks[5] = 0.0 # set step 6 to be 0.0, indicating end of episode 1.\n",
    "next_value_case_1 = 1.0 # next step is termination for episode 2.\n",
    "next_value_case_2 = 10.0 # next step is not termination for episode 2.\n",
    "\n",
    "returns_case_1 = compute_discounted_return(rewards, masks, gamma, next_value_case_1)\n",
    "returns_case_2 = compute_discounted_return(rewards, masks, gamma, next_value_case_2)\n",
    "print('return_case_1 is {} \\nreturn_case_2 is {}'.format(returns_case_1, returns_case_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generalized Advantage Estimate ([GAE](https://arxiv.org/pdf/1506.02438.pdf))\n",
    "\n",
    "See reference for GAE [here](https://arxiv.org/pdf/1506.02438.pdf).\n",
    "\n",
    "$GAE(\\gamma,\\lambda)$ is exponential-weight average of k-step Advantage estimators (Huh?). It's important to note here what exactly is Advantage $A^{\\pi,\\gamma}(s_t,a_t)$. **FYI:** $A^{\\pi,\\gamma}(s_t,a_t)$ reads as the Advantage ($A$) estimate for being in state ($s_t$), taking action ($a_t$), while following policy ($\\pi$) and using discount factor ($\\gamma$).\n",
    "\n",
    "Formally: $A^{\\pi,\\gamma}(s_t,a_t) = Q^{\\pi,\\gamma}(s_t,a_t) - V^{\\pi,\\gamma}(s_t,a_t)$\n",
    "\n",
    "Where $Q^{\\pi,\\gamma}(s_t,a_t)$ is the state-action value function (Also called Q-function) and $ V^{\\pi,\\gamma}(s_t,a_t)$ is the state value function (Also called V-function).\n",
    "\n",
    "Intuitively (subjective): Q-function estimates the value of being in some state and taking some action, where as V-function estimates the value of being in some state, by taking A = Q - V, we say Advantage estimate the advantage of the action taking given being in some state.\n",
    "\n",
    "### Temporal Difference Residual for V\n",
    "\n",
    "Previously, by the definition of A, it would seem that we need to have both Q and V functions to estimate A. We can simply this a bit by considering the fact that given a sufficient accurate V-function, we get a biased estimate for our advantage using only V-funcion alone(via the TD residual ):\n",
    "\n",
    "$\\mathbb{E}_{s_{t+1}}\\left[ \\hat{A}_{t} \\right] = \\mathbb{E}_{s_{t+1}} $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
